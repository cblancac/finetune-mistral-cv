cv_text = """Working Experience
Miguel Carlos
Blanco
Cacharr√≥n
2023 ‚Äì
Present
(Remote)Consultancy | Start-up
Senior AI Data Scientist at Turing Challenge
Creation of QnA and Chatbots systems using vector databases
(Pinecone, Elasticsearch or Azure Cognitive Search), RAG and LLM
models (GPT3.5 or GPT4). Fine-tuned of LLMs, like GPT-3.5 Turbo
or Llama, to improve the performance of GPT4, reducing prices up
to 20x lower for inference. Client: Repsol, Microsoft and Mediapro.
2022 ‚Äì 2023
(Remote)Consultancy | Start-up
Data Scientist Specialist at Foqum Analytics
Multi-label text classification with FastAi or Hugging Face (using
longformers). NER using regex for entities with fixed patterns,
and Hugging Face, in another case. Use of Haystack to build QA
pipelines, storing the documents in Elasticsearch. Client: Lefebvre.
2020 ‚Äì 2022
(Hybrid)Consultancy
Engineer NLP at NTT Data
Extract texts from different types of documents using AWS Textract.
Classify them by using Neural Networks, SVM, Naive Bayes or Decis-
sion Trees. Get the entities present in the documents (Regex, Spacy
and TextBlob). Use of Microsoft Azure (Repos, Pipelines). Use of
Jira as an agile methodology. Clients: Iberdrola Spain, Vivo Brazil
and Entel Chile.
2019 ‚Äì 2020
(Presential)Consultancy
Data Scientist at Strategy Big Data
Computer Vision - Improving the quality of the text extracted using
free OCR like pytesseract. Bayesian Optimizers to choose the best
combinations of hyperparameters.
Natural Language Processing - Extract the entities required in the
documents. Client: Banco Santander.
Data Scientist Specialist
i April 11, 1992
ƒá Gines, Sevilla (Spain)
√ó +34 603 468 505
∆ü carblacac7@gmail.com
~ Spanish
Social Network
]
a
Education
Study
LinkedIn
2018 ‚Äì 2019
Github
Hugging Face
Languages
2013 ‚Äì 2018
Spanishƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Englishƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Frenchƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Skills
CSIC and UIMP
Master in Data Science
Focus: Machine Learning and Deep Learning with Python and R.
Techniques of NLP, CV, Web Scrapping and ETL. Use of relational
and non-relational databases.
Master Theses
Sentiment analysis on Twitter, applying different Deep Learning
state-of-the-art techniques like transformers (BERT) using Python.
UNED
Degree in Mathematics
Focus: Programming (R and C++) applying knowledge of mathemat-
ics and statistics.
Bachelor Theses
Linear models in high dimensional small sampled datasets: Applied
to real data on Colon Cancer using R.
Courses
Programming:
Pythonƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Bash scriptingƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Rƒ™ ƒ™ ƒ™ ƒ™ ƒ™
MATLAB, C++ƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Tools:
(Click the image)
Hugging Faceƒ™ ƒ™ ƒ™ ƒ™ ƒ™Certifications
TensorFlow, Pytorchƒ™ ƒ™ ƒ™ ƒ™ ƒ™Microsoft
LangChainƒ™ ƒ™ ƒ™ ƒ™ ƒ™Spacy, TextBlobƒ™ ƒ™ ƒ™ ƒ™ ƒ™OpenCVƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Microsoft Certified: Azure AI Engineer Associate
2023Miguel Carlos
Blanco
Cacharr√≥n
Data Scientist Specialist
Books of interest
O‚ÄôReilly
Packt
Natural Language with Transformers. Building Language Applica-
tions with Hugging Face
Lewis Tunstall, Leandro von Werra & Thomas Wolf
2022
Mastering spaCy: An end-to-end practical guide to implementing
NLP applications using the Python ecosystem
Duygu Altinok
2021
Deep Learning for Coders with Fastai and Pytorch: AI Applications
Without a PhD
Howard, J. and Gugger, S.
2020
About MeO‚ÄôReilly
I‚Äôm a person who enjoy with the small
details of the life, learning new things,
trying different foods and visiting new
places. The sport, family and friends
make my life easier.Personal Projects
A Normal Week2021
Program
Sport
2020
Sleep
Family
2019
Miguel Carlos
Blanco
Cacharr√≥n
2023 ‚Äì
Present
(Remote)Consultancy | Start-up
Senior AI Data Scientist at Turing Challenge
Creation of QnA and Chatbots systems using vector databases
(Pinecone, Elasticsearch or Azure Cognitive Search), RAG and LLM
models (GPT3.5 or GPT4). Fine-tuned of LLMs, like GPT-3.5 Turbo
or Llama, to improve the performance of GPT4, reducing prices up
to 20x lower for inference. Client: Repsol, Microsoft and Mediapro.
2022 ‚Äì 2023
(Remote)Consultancy | Start-up
Data Scientist Specialist at Foqum Analytics
Multi-label text classification with FastAi or Hugging Face (using
longformers). NER using regex for entities with fixed patterns,
and Hugging Face, in another case. Use of Haystack to build QA
pipelines, storing the documents in Elasticsearch. Client: Lefebvre.
2020 ‚Äì 2022
(Hybrid)Consultancy
Engineer NLP at NTT Data
Extract texts from different types of documents using AWS Textract.
Classify them by using Neural Networks, SVM, Naive Bayes or Decis-
sion Trees. Get the entities present in the documents (Regex, Spacy
and TextBlob). Use of Microsoft Azure (Repos, Pipelines). Use of
Jira as an agile methodology. Clients: Iberdrola Spain, Vivo Brazil
and Entel Chile.
2019 ‚Äì 2020
(Presential)Consultancy
Data Scientist at Strategy Big Data
Computer Vision - Improving the quality of the text extracted using
free OCR like pytesseract. Bayesian Optimizers to choose the best
combinations of hyperparameters.
Natural Language Processing - Extract the entities required in the
documents. Client: Banco Santander.
Data Scientist Specialist
i April 11, 1992
ƒá Gines, Sevilla (Spain)
√ó +34 603 468 505
∆ü carblacac7@gmail.com
~ Spanish
Social Network
]
a
Education
Study
LinkedIn
2018 ‚Äì 2019
Github
Hugging Face
Languages
2013 ‚Äì 2018
Spanishƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Englishƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Frenchƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Skills
CSIC and UIMP
Master in Data Science
Focus: Machine Learning and Deep Learning with Python and R.
Techniques of NLP, CV, Web Scrapping and ETL. Use of relational
and non-relational databases.
Master Theses
Sentiment analysis on Twitter, applying different Deep Learning
state-of-the-art techniques like transformers (BERT) using Python.
UNED
Degree in Mathematics
Focus: Programming (R and C++) applying knowledge of mathemat-
ics and statistics.
Bachelor Theses
Linear models in high dimensional small sampled datasets: Applied
to real data on Colon Cancer using R.
Courses
Programming:
Pythonƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Bash scriptingƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Rƒ™ ƒ™ ƒ™ ƒ™ ƒ™
MATLAB, C++ƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Tools:
(Click the image)
Hugging Faceƒ™ ƒ™ ƒ™ ƒ™ ƒ™Certifications
TensorFlow, Pytorchƒ™ ƒ™ ƒ™ ƒ™ ƒ™Microsoft
LangChainƒ™ ƒ™ ƒ™ ƒ™ ƒ™Spacy, TextBlobƒ™ ƒ™ ƒ™ ƒ™ ƒ™OpenCVƒ™ ƒ™ ƒ™ ƒ™ ƒ™
Microsoft Certified: Azure AI Engineer Associate
2023Miguel Carlos
Blanco
"""

import openai
import json
import time
import logging

from transformers import AutoTokenizer

# Configuraci√≥n b√°sica de logging con timestamps
logging.basicConfig(
    format="%(asctime)s %(levelname)s %(message)s",
    level=logging.INFO,
    datefmt="%Y-%m-%d %H:%M:%S"
)
logger = logging.getLogger(__name__)

# NEW: load the same tokenizer you used for fine-tuning / merge
TOKENIZER = AutoTokenizer.from_pretrained(
    "checkpoints/mistral-cv-merged-final",        # local folder or HF repo
    trust_remote_code=True,
)
MAX_INPUT_TOKENS = 1524

def truncate_text(text: str) -> str:  # ‚Üê NEW
    """
    Keep only the first MAX_INPUT_TOKENS tokens of `text`
    (counted with the Mistral tokenizer) and decode back to UTF-8.
    """
    ids = TOKENIZER(text, add_special_tokens=False)["input_ids"]
    if len(ids) > MAX_INPUT_TOKENS:
        ids = ids[:MAX_INPUT_TOKENS]
    return TOKENIZER.decode(
        ids,
        skip_special_tokens=True,
        clean_up_tokenization_spaces=True,
    )

# 1Ô∏è‚É£ Medir tiempo de inicializaci√≥n del cliente
client_init_start = time.time()
logger.info("Initializing OpenAI client for vLLM...")
client = openai.OpenAI(
    base_url="http://localhost:8000/v1",
    api_key="EMPTY"
)
client_init_end = time.time()
logger.info(f"Client initialized in {client_init_end - client_init_start:.3f}s")

# El esquema que has definido
SCHEMA = json.dumps({
    "certifications": "",
    "contact_detail": {
        "age": "",
        "email": "",
        "home_city": "",
        "mobile": "",
        "name": ""
    },
    "education": [{"degree": "", "degree_level": "", "end_date": "", "school_name": "", "start_date": ""}],
    "gender": "",
    "industry": "",
    "skills": [],
    "software_tools": [],
    "work_abroad": "",
    "work_experience": [{"company": "", "end_date": "", "position": "", "start_date": ""}]
}, separators=(",", ":"))  

# Prompt del sistema
SYSTEM_PROMPT = (
    "You are an API that extracts structured JSON from resumes.\n"
    "Return *only* valid JSON matching exactly this schema:\n"
    f"{SCHEMA}"
)

def extract_cv(cv_text):
    # 2Ô∏è‚É£ Antes de la llamada a la inferencia
    logger.info("Preparing to send chat completion request...")

    # NEW: truncate before the call
    tokenizer_start = time.time()
    cv_text = truncate_text(cv_text)
    tokenizer_end = time.time()
    logger.info(f"Tokenization takes {tokenizer_end - tokenizer_start:.3f}s")

    request_start = time.time()
    response = client.chat.completions.create(
        model="mistral-cv-merged-final",
        messages=[
            {"role": "system", "content": SYSTEM_PROMPT},
            {"role": "user", "content": cv_text}
        ],
        max_tokens=888,
        temperature=0.0,
    )
    
    # 3Ô∏è‚É£ Despu√©s de recibir la respuesta
    request_end = time.time()
    logger.info(f"Chat completion request took {request_end - request_start:.3f}s")

    return response.choices[0].message.content

from concurrent.futures import ThreadPoolExecutor, as_completed

def extract_cv_wrapper(cv_text):
    return extract_cv(cv_text)

if __name__ == "__main__":
    LOOPS          = 20          # ‚Üê n¬∫ de pasadas completas
    PARALLEL_JOBS  = 5           # ‚Üê hilos concurrentes por pasada
    CVs_PER_JOB    = 1           # ‚Üê cu√°ntos CV procesa cada hilo

    # Pre-construye la lista de curr√≠culos para cada pasada
    batch_cvs = [cv_text] * (PARALLEL_JOBS * CVs_PER_JOB)

    global_start = time.time()
    for loop in range(1, LOOPS + 1):
        logger.info(f"===== RUN {loop}/{LOOPS} =====")

        start = time.time()
        results = []

        with ThreadPoolExecutor(max_workers=PARALLEL_JOBS) as ex:
            futures = [ex.submit(extract_cv_wrapper, cv) for cv in batch_cvs]

            for future in as_completed(futures):
                try:
                    results.append(future.result())
                except Exception as e:
                    logger.error(f"Error during CV extraction: {e}")

        dur = time.time() - start
        logger.info(f"Run {loop} finished in {dur:.2f} s "
                    f"(processed {len(results)} CVs)")

        # (Opcional) imprime las primeras respuestas de la pasada
        for idx, res in enumerate(results[:3], start=1):
            print(f"\n=== Run {loop} ¬∑ Result {idx} ===\n{res}")

    logger.info(f"üèÅ Completed {LOOPS} full runs in "
                f"{time.time() - global_start:.2f} seconds")